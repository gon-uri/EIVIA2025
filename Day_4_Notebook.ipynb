{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awBLy-xuFdgX"
      },
      "outputs": [],
      "source": [
        "!pip install scipy==1.14.0\n",
        "!pip install sktime --quiet\n",
        "!pip install pyts --quiet\n",
        "!pip install tsfresh --quiet\n",
        "!pip install git+https://github.com/gon-uri/detach_rocket --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJsfBi2tFdga"
      },
      "source": [
        "## 1) Download and Prepare Data: NOW MULTIVARIATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-A3s151Fdgb"
      },
      "source": [
        "### Download a dataset from the UEA archive\n",
        "\n",
        "Instead of using the UCR, we will use the UEA, the archive for multivariate time series.\n",
        "You can still get the description of the datasets in the following webpage: https://www.timeseriesclassification.com/dataset.php"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TlygVr0Fdgb"
      },
      "outputs": [],
      "source": [
        "# We will use the UCR dataset\n",
        "\n",
        "from detach_rocket.utils_datasets import fetch_uea_dataset\n",
        "from pyts.datasets import uea_dataset_list\n",
        "\n",
        "# Download Dataset\n",
        "selected_dataset = ['SelfRegulationSCP1'] # SelfRegulationSCP1\n",
        "print(\"Selected dataset:\", selected_dataset)\n",
        "current_dataset = fetch_uea_dataset(selected_dataset[0])\n",
        "\n",
        "print(\" \")\n",
        "# You can later try other UCR datasets (First use WormsTwoClass dataset)\n",
        "dataset_list = uea_dataset_list()\n",
        "print(\"All UEA Datasets:\", dataset_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lUa6-o8-rdK"
      },
      "outputs": [],
      "source": [
        "# Function to convert all labels to 0,1,2,3,4,etc..\n",
        "\n",
        "def convert_labels_to_integers(labels):\n",
        "    # Create a mapping of unique labels to integers\n",
        "    unique_labels = sorted(set(labels))\n",
        "    label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "    # Convert labels using the mapping\n",
        "    converted_labels = np.asarray([label_to_int[label] for label in labels])\n",
        "    return converted_labels, label_to_int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZmK9UoCFdgc"
      },
      "source": [
        "### Information about the Dataset\n",
        "**About the *SelfRegulationSCP1* dataset:** The dataset contains EEG recordings from a healthy subject performing a cursor control task through self-regulation of slow cortical potentials (SCPs), with 268 training trials and 293 test trials, each represented by six EEG channels recorded at 256 Hz for 3.5 seconds, and labeled based on cortical positivity or negativity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwgiXngDFdgc"
      },
      "source": [
        "### Create the corresponding Train and Test matrices\n",
        "\n",
        "We unpack the dictionary downloaded form the UCR into the train and test matrices that we will use for training our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0dVMuZfFdgc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create data matrices and remove possible rows with nans or infs\n",
        "print(f\"Dataset Matrix Shape: ( # of instances , # of channels , time series length )\")\n",
        "print(f\" \")\n",
        "\n",
        "# Train Matrix\n",
        "X_train = current_dataset['data_train'].copy()\n",
        "\n",
        "# NOTICE TAHAT THIS PART IS DIFFERENT FOR THE MULTIVARIATE CASE\n",
        "# This part is to make shure there is no nans of inf in the data\n",
        "non_nan_mask_train = ~np.isnan(~np.isnan(X_train).any(axis=1)).any(axis=1) # We have to check in both dimensions (channels and timesteps)\n",
        "non_inf_mask_train = ~np.isnan(~np.isnan(X_train).any(axis=1)).any(axis=1) # We have to check in both dimensions (channels and timesteps)\n",
        "mask_train = np.logical_and(non_nan_mask_train,non_inf_mask_train)\n",
        "X_train = X_train[mask_train]\n",
        "\n",
        "# Shuffle dataset to avoid any possible bias (keep indexes to shuffle the labels)\n",
        "np.random.seed(42)\n",
        "indexes = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(indexes)\n",
        "X_train = X_train[indexes]\n",
        "\n",
        "print(f\"Train Matrix Shape: {X_train.shape}\")\n",
        "\n",
        "# We create the labels\n",
        "y_train = current_dataset['target_train'].copy()\n",
        "\n",
        "# Conver the labels to integers starting from zero\n",
        "y_train, label_to_int = convert_labels_to_integers(y_train)\n",
        "\n",
        "# Remove nans also form labels\n",
        "y_train = y_train[mask_train]\n",
        "\n",
        "# Shuffle the labels in the same way as the data\n",
        "y_train = y_train[indexes]\n",
        "\n",
        "print(f\" \")\n",
        "\n",
        "# Test Matrix\n",
        "X_test = current_dataset['data_test'].copy()\n",
        "\n",
        "# NOTICE TAHAT THIS PART IS DIFFERENT FOR THE MULTIVARIATE CASE\n",
        "# This part is to make shure there is no nans of inf in the data\n",
        "non_nan_mask_test = ~np.isnan(~np.isnan(X_test).any(axis=1)).any(axis=1) # We have to check in both dimensions (channels and timesteps)\n",
        "non_inf_mask_test = ~np.isnan(~np.isnan(X_test).any(axis=1)).any(axis=1) # We have to check in both dimensions (channels and timesteps)\n",
        "mask_test = np.logical_and(non_nan_mask_test,non_inf_mask_test)\n",
        "X_test = X_test[mask_test]\n",
        "\n",
        "print(f\"Test Matrix Shape: {X_test.shape}\")\n",
        "\n",
        "y_test = current_dataset['target_test'].copy()\n",
        "\n",
        "# Remove nans also form labels\n",
        "y_test = y_test[mask_test]\n",
        "\n",
        "# Conver the labels to integers starting from zero\n",
        "y_test = np.asarray([label_to_int[label] for label in y_test])\n",
        "\n",
        "# Print the different unique classes\n",
        "print(f\" \")\n",
        "print(f\"Unique Classes: {np.unique(y_train)}\")\n",
        "number_of_classes = len(np.unique(y_train))\n",
        "\n",
        "# Print the proportion of each class\n",
        "print(f\" \")\n",
        "for i in np.unique(y_train):\n",
        "    print(f\"Class {i} has {np.sum(y_train==i)/len(y_train)*100:.2f}% of the data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBUJP7g_Fdgc"
      },
      "source": [
        "### Plotting the time series data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxgILF-cFdgd"
      },
      "outputs": [],
      "source": [
        "# Plot the 10 instances in a subplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the first 10 instances of the multivariate time series data of shape (num_instances, num_channels, num_timesteps)\n",
        "plt.figure(figsize=(18,10))\n",
        "for i in range(10):\n",
        "    plt.subplot(2,5,i+1)\n",
        "\n",
        "    for j in range(X_train.shape[1]):\n",
        "\n",
        "        # We add an offset to the data to make it more readable\n",
        "        offset = 10\n",
        "        plt.plot(X_train[i,j,:]+10*j, label=f\"Channel {j}\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.title(f\"Class {y_train[i]}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQMDuY1UZmk2"
      },
      "source": [
        "Notice that the values in each EEG channel are not exactly the same, but they are highly correlated!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJNIrF7dYepr"
      },
      "source": [
        "## 2) ROCKET classifier\n",
        "\n",
        "A Rocket classifier consists in two stages. The first one is to convert the data using the Rocket transformation. The second one is to classify that data using a regularized ridge classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbUjjjx3TfzR"
      },
      "outputs": [],
      "source": [
        "from sktime.transformations.panel.rocket import MultiRocketMultivariate, MiniRocketMultivariate\n",
        "\n",
        "# First Stage: Transformation\n",
        "\n",
        "# Create the transformation\n",
        "np.random.seed(1)\n",
        "rocket_trf = MultiRocketMultivariate()\n",
        "\n",
        "# Fit the transformation\n",
        "rocket_trf.fit(X_train)\n",
        "\n",
        "# Transform the data\n",
        "rocket_X_train = rocket_trf.transform(X_train)\n",
        "rocket_X_test = rocket_trf.transform(X_test)\n",
        "\n",
        "# Print the shape of the new dataset\n",
        "print(f\"Rocket Features Matrix Shape: ( # of instances , # of features )\")\n",
        "print(f\" \")\n",
        "print(f\"Train: {rocket_X_train.shape}\")\n",
        "print(f\" \")\n",
        "print(f\"Test: {rocket_X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnXA2leBZAhQ"
      },
      "source": [
        "Lets now use this very large amount of features to classify. We will use a Ridge classifier, which is a lenar classifier with l2 regularization. With the Ridge Classifier we can implemente a leave-one-out cross validation (LOOCV) proccess almost \"for free\" in terms of proccesing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptlMm0wcTkUu"
      },
      "outputs": [],
      "source": [
        "# Second Stage : Calssification\n",
        "\n",
        "# Train a ridge classifier on the data\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# We create a Ridge Classifier to perform the classification task\n",
        "rocket_classifier = RidgeClassifierCV(alphas=np.logspace(-10,10,20), class_weight='balanced')\n",
        "\n",
        "rocket_classifier.fit(rocket_X_train, y_train)\n",
        "\n",
        "# Predict on the train set\n",
        "y_pred_train = rocket_classifier.predict(rocket_X_train)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_train = accuracy_score(y_train,y_pred_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_test = rocket_classifier.predict(rocket_X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_test = accuracy_score(y_test,y_pred_test)\n",
        "\n",
        "# Print Accuracy\n",
        "print(f\"Accuracy Train = {accuracy_train:.2f}\")\n",
        "print(f\"Accuracy Test = {accuracy_test:.2f}\")\n",
        "\n",
        "print(f\" \")\n",
        "print(f\"Confusion Matrix on Test Set: \")\n",
        "# Plot confusion matrix\n",
        "conf_matrix_test = confusion_matrix(y_test,y_pred_test)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_test,display_labels=np.unique(y_test))\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Exercises:\n",
        "\n",
        "1) The channels in the EEG data set are highly correlated. What does this mean? Is it really necessary to have all the channels to classify? Try to answer this question by subsampling the channels (selecting only a subset of channels) and see if this affects your prediction accuracy (or not).\n",
        "\n",
        "2) Try using Minirocket instead of Multirocket. Compare the number of features generated by each model. Is there a difference in accuracy for the classification task?\n",
        "\n",
        "3) Try different multivariate datasets from the UEA archive.\n",
        "\n",
        "4) Now implement a calsifier on the data using Catch22. Compare its accuracy with that of the ROCKET classifier. Tip: Reuse the code from the previous notebooks.\n",
        "\n",
        "5) You can also try to run the CNN models below.\n"
      ],
      "metadata": {
        "id": "Az6kKg6jkPIf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bviOnOOzFdgf"
      },
      "source": [
        "## [EXTRA] 4) CNN Model\n",
        "We will train a vanilla CNN architecture to solve the classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUMsDHuGFdgf"
      },
      "outputs": [],
      "source": [
        "# The definition of these callbacks is just to store the accuracy values during training process.\n",
        "# There is no need to modify or understand this part of the code.\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acc_list_train = []\n",
        "acc_list_test = []\n",
        "\n",
        "class AccuracyCallbackTrain(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X, y, threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.X = np.swapaxes(X, 1, 2) # Correct the shape if necessary\n",
        "        self.y = y\n",
        "        self.threshold = threshold  # Threshold for binary classification (default 0.5)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # Get predictions on the entire training set\n",
        "        predictions = self.model.predict(self.X)\n",
        "        y_pred = predictions.argmax(axis=1)\n",
        "        accuracy = accuracy_score(self.y, y_pred)\n",
        "\n",
        "        # Store accuracy value\n",
        "        acc_list_train.append(accuracy)\n",
        "        #print(f\"Epoch {epoch + 1}: Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "class AccuracyCallbackTest(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X, y, threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.X = np.swapaxes(X, 1, 2) # Correct the shape if necessary\n",
        "        self.y = y\n",
        "        self.threshold = threshold  # Threshold for binary classification (default 0.5)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # Get predictions on the entire training set\n",
        "        predictions = self.model.predict(self.X)\n",
        "        y_pred = predictions.argmax(axis=1)\n",
        "        accuracy = accuracy_score(self.y, y_pred)\n",
        "\n",
        "        # Store accuracy value\n",
        "        acc_list_test.append(accuracy)\n",
        "        #print(f\"Epoch {epoch + 1}: Accuracy = {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TYBx77JFdgf"
      },
      "source": [
        "Let's train the simple CNN model on the raw time series data.\n",
        "\n",
        "By default, this model will posses the following hyperparameters:\n",
        "\n",
        "*   kernel_size = 7\n",
        "*   avg_pool_size = 3\n",
        "*   n_conv_layers = 2\n",
        "*   filter_sizes = [6, 12] (this is the number of kernelsper layer, need to be equal to the number of layers)\n",
        "\n",
        "You can read more in depth about it in the corresponding documentation:\n",
        "https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.classification.deep_learning.CNNClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYPCEZczFdgf"
      },
      "outputs": [],
      "source": [
        "from sktime.classification.deep_learning import CNNClassifier\n",
        "\n",
        "# Initialize the callbacks (this is to keep track of the learning process while training)\n",
        "train_accuracy_callback = AccuracyCallbackTrain(X_train, y_train)\n",
        "test_accuracy_callback = AccuracyCallbackTest(X_test, y_test)\n",
        "acc_list_train = []\n",
        "acc_list_test = []\n",
        "\n",
        "# Create the classifier\n",
        "cnn_classifier = CNNClassifier(\n",
        "\n",
        "    # PARAMETERS\n",
        "    n_epochs=50, # Number of epochs\n",
        "    #kernel_size = 15,\n",
        "    #avg_pool_size = 5,\n",
        "    #n_conv_layers = 3,\n",
        "    #filter_sizes = [6, 6, 6],\n",
        "\n",
        "    verbose=True, # Print information about the training\n",
        "    callbacks=[train_accuracy_callback,test_accuracy_callback] # Call the custom callbacks we have created\n",
        "    )\n",
        "\n",
        "# Train the model\n",
        "cnn_classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1cGh_cBXhBW"
      },
      "outputs": [],
      "source": [
        "# Plot both the loss and the accuracies (train and test) in the same graph using two different y-axis.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss', color=color)\n",
        "ax1.plot(cnn_classifier.history.history['loss'], color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('Accuracy', color=color)\n",
        "ax2.plot(acc_list_train, color=color, linestyle='dashed', label='Train Accuracy')\n",
        "ax2.plot(acc_list_test, color=color, linestyle='solid', label='Test Accuracy')\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.legend(loc=\"upper right\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhwGPoOlFdgf"
      },
      "outputs": [],
      "source": [
        "# Compute and print the accuracy on the train and test set using the final model\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "y_test_pred = cnn_classifier.predict(X_test)\n",
        "y_train_pred = cnn_classifier.predict(X_train)\n",
        "\n",
        "# Predict the train set\n",
        "y_train_pred = cnn_classifier.predict(X_train)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_train = accuracy_score(y_train,y_train_pred)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_test = accuracy_score(y_test,y_test_pred)\n",
        "\n",
        "print(f\"Accuracy Train = {accuracy_train:.2f}\")\n",
        "print(f\"Accuracy Test = {accuracy_test:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbXzAboTLZCv"
      },
      "source": [
        "## [EXTRA] 5) InceptionTime Classifier\n",
        "\n",
        "Let us now train an IncpetionTime classifier. The original IncpetionTime model consists of an ensemble of 5 individual IncpetionTime networks, here we are just training one. An ensemble is used to counteract the fact that the individual IncpetionTime networks tend to have a lot of variability (some realisations of the model perform really well and others dont)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras implementation used in SKTime is not ideal, it will take a lot to train, and it doesnt handle very well the BatchNormalization curves when using for evaluation."
      ],
      "metadata": {
        "id": "Lp7BAvNMlF8v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaIHqUDLWyfa"
      },
      "outputs": [],
      "source": [
        "from sktime.classification.deep_learning import InceptionTimeClassifier\n",
        "\n",
        "batch_size = 64\n",
        "inctime = InceptionTimeClassifier(n_epochs=10,batch_size=batch_size, verbose=True)\n",
        "inctime.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy8aZfTAHz1V"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Create a dummy batch to force update BN statistics\n",
        "dummy_input = np.copy(X_train[:batch_size])  # Assuming batch size\n",
        "_ = inctime.predict(dummy_input)  # This updates the internal BN statistics\n",
        "\n",
        "y_train_pred = inctime.predict(X_train)\n",
        "y_test_pred = inctime.predict(X_test)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "forecast",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}